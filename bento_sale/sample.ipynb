{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "# define input string\n",
    "data = 'hello world'\n",
    "print(data)\n",
    "# define universe of possible input values\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[char] for char in data]\n",
    "print(integer_encoded)\n",
    "# one hot encode\n",
    "onehot_encoded = list()\n",
    "for value in integer_encoded:\n",
    "\tletter = [0 for _ in range(len(alphabet))]\n",
    "\tletter[value] = 1\n",
    "\tonehot_encoded.append(letter)\n",
    "print(onehot_encoded)\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.Series([\"apple\",\"amazon\",\"google\",\"facebook\",\"microsoft\",\"apple\",\"apple\",\"amazon\",\"amazon\",\"google\", None])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data = pd.unique(data)\n",
    "print(unique_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int = dict((c, i) for i, c in enumerate(unique_data))\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(unique_data))\n",
    "int_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "label_encoded = [char_to_int[char] for char in data]\n",
    "print(label_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one-hot encoding\n",
    "integer_encoded = [char_to_int[char] for char in data]\n",
    "onehot_encoded = list()\n",
    "for value in integer_encoded:\n",
    "\tletter = [0 for _ in range(len(unique_data))]\n",
    "\tletter[value] = 1\n",
    "\tonehot_encoded.append(letter)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#binary encoding\n",
    "integer_encoded = [char_to_int[char] for char in data]\n",
    "binary_encoding = map(lambda x: list(\"{0:b}\".format(x).zfill(10+1)), integer_encoded)\n",
    "print(list(binary_encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--実行モジュール.py or .ipynb\n",
    "--my_modules(パッケージ)\n",
    "  --init.py(パッケージを再帰的に読み込むためのダミーファイル)\n",
    "  --my_module.py(module)\n",
    "    --my_class(class)\n",
    "      --my_def(function)\n",
    "\n",
    "1.パッケージはimportできない \n",
    "import my_modules\n",
    "->error\n",
    "\n",
    "2.moduleはimportできる fromを使用してもよい \n",
    "import my_modules.my_module\n",
    "-> ok\n",
    "\n",
    "from my_modules import my_module\n",
    "-> ok\n",
    "\n",
    "3.classはimportできるが、fromを使用しないといけない(.でパスを指定できない)\n",
    "import my_modules.my_module.myclass\n",
    "-> error\n",
    "\n",
    "from my_modules.my_module import myclass\n",
    "-> ok\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from my_modules.my_encoder import CategoryValueEncoder as CVE\n",
    "\n",
    "data = pd.Series([\"apple\",\"amazon\",\"google\",\"facebook\",\"microsoft\",\"apple\",\"apple\",\"amazon\",\"amazon\",\"google\", None])\n",
    "encoder = CVE(data)\n",
    "encoded = encoder.to_one_hot_encoding()\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import my_modules.my_encoder as my_encoder\n",
    "\n",
    "data = pd.Series([\"apple\",\"amazon\",\"google\",\"facebook\",\"microsoft\",\"apple\",\"apple\",\"amazon\",\"amazon\",\"google\", None])\n",
    "encoder = my_encoder.CategoryValueEncoder(data)\n",
    "encoded = encoder.to_binary_encoding()\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from my_modules import my_encoder\n",
    "\n",
    "data = pd.Series([\"apple\",\"amazon\",\"google\",\"facebook\",\"microsoft\",\"apple\",\"apple\",\"amazon\",\"amazon\",\"google\", None])\n",
    "encoder = my_encoder.CategoryValueEncoder(data)\n",
    "encoded = encoder.to_binary_encoding()\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from my_modules import my_encoder\n",
    "\n",
    "data = pd.Series([\"apple\",\"amazon\",\"google\",\"facebook\",\"microsoft\",\"apple\",\"apple\",\"amazon\",\"amazon\",\"google\", None], name = 'company_name')\n",
    "encoder = my_encoder.CategoryValueEncoder(data)\n",
    "encoded = encoder.to_binary_encoding()\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "fluits = [\"apple & banana apple\",\"maron\",\"melon\",\"grape & apple\",\"lemon\",\"melon & maron\",\"orange\",\"queie\",\"orange\"]\n",
    "data = pd.Series(fluits, name = 'fruits_name')\n",
    "\n",
    "series_name = 'fruits_name'\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "corpus = data.values\n",
    "\n",
    "#print(data.values)\n",
    "\n",
    "series = vectorizer.fit_transform(corpus)\n",
    "feature_name = vectorizer.get_feature_names()\n",
    "print(feature_name)\n",
    "print(series.toarray())\n",
    "print(type(series))\n",
    "print(type(series.toarray()))\n",
    "print(type(list(series)))\n",
    "\n",
    "dataframe = pd.DataFrame(series.toarray())\n",
    "dataframe = dataframe.rename(lambda x: series_name + '_enc_' + str(x), axis='columns')\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([[1,2,3,4,5,6],\n",
    "                       [6,5,4,3,2,1],\n",
    "                       [100,1,2,3,4,5],\n",
    "                       [5,4,3,2,1,0]], columns=[*\"ABCDEF\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x = df['A']\n",
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df_x.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = df_x.std(ddof=0)\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#norm = df_x.map(lambda x: round((x - mean) / std)).astype(int)\n",
    "norm = df_x.map(lambda x: round((x - mean) / std / 10 + 0.5 , 2)).astype(float)\n",
    "norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./Data/train.csv\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_modules.my_encoder import TextValueEncoder as TVE\n",
    "df = TVE(train_df['project_subject_subcategories'])\n",
    "df.to_bow_encoding()\n",
    "vec = df.vectorizer\n",
    "names = vec.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = len(names)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                    'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "                    'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "                    'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "                   index=[0, 1, 2, 3])\n",
    "\n",
    "df2 = pd.DataFrame({'A': ['A4', 'A5', 'A6', 'A7'],\n",
    "                    'B': ['B4', 'B5', 'B6', 'B7'],\n",
    "                    'C': ['C4', 'C5', 'C6', 'C7'],\n",
    "                    'D': ['D4', 'D5', 'D6', 'D7']},\n",
    "                   index=[4, 5, 6, 7])\n",
    "\n",
    "df3 = pd.DataFrame({'A': ['A8', 'A9', 'A10', 'A11'],\n",
    "                    'B': ['B8', 'B9', 'B10', 'B11'],\n",
    "                    'C': ['C8', 'C9', 'C10', 'C11'],\n",
    "                    'D': ['D8', 'D9', 'D10', 'D11']},\n",
    "                   index=[8, 9, 10, 11])\n",
    "\n",
    "df4 = pd.DataFrame({'B': ['B2', 'B3', 'B6', 'B7'],\n",
    "                    'D': ['D2', 'D3', 'D6', 'D7'],\n",
    "                    'F': ['F2', 'F3', 'F6', 'F7']},\n",
    "                   index=[2, 3, 6, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condat1 = pd.concat([df1, df4], axis=1)\n",
    "condat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condat2 = pd.concat([df1, df4], axis=1, join_axes=[df1.index])\n",
    "condat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "stack = pd.DataFrame({'id': ['A01', 'A01', 'A02', 'A03'],\n",
    "                     'description': ['apple', 'amazon', 'google', 'facebook'],\n",
    "#                     'description': ['1', '2', '3', '4'],\n",
    "                     'quantity': ['3', '2', '4', '10'],\n",
    "                     'price': ['80', '100', '90', '70']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price = pd.Series(stack['quantity'].fillna(0.0).astype(int) * stack['price'].fillna(0.0).astype(int))\n",
    "price = price.rename('total_price', axis='columns')\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = pd.concat([stack, price], axis=1)\n",
    "stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = stack.groupby('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def f_reduce_sum(list):\n",
    "    return functools.reduce(lambda x, y: x + y, list)\n",
    "\n",
    "#ans = f_reduce_sum(range(1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_reduce_concat(list):\n",
    "    return functools.reduce(lambda x, y: x + y, list)\n",
    "\n",
    "#ans = f_reduce_concat(['abc','def','ghi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped.agg({'description': f_reduce_concat,\n",
    "             'total_price': f_reduce_sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "left = pd.DataFrame({'id': ['K0', 'K1', 'K2', 'K3'],\n",
    "                     'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "                     'B': ['B0', 'B1', 'B2', 'B3']})\n",
    "\n",
    "right = pd.DataFrame({'id': ['K1', 'K3', 'K5', 'K7'],\n",
    "                      'C': ['C1', 'C3', 'C5', 'C7'],\n",
    "                      'D': ['D1', 'D3', 'D5', 'D7']},\n",
    "                     index=[1, 3, 5, 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(left, right, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from xgboost import XGBClassifier\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 78035 entries, 0 to 78034\n",
      "Data columns (total 15 columns):\n",
      "id                                              78035 non-null object\n",
      "teacher_id                                      78035 non-null object\n",
      "teacher_prefix                                  78034 non-null object\n",
      "school_state                                    78035 non-null object\n",
      "project_submitted_datetime                      78035 non-null object\n",
      "project_grade_category                          78035 non-null object\n",
      "project_subject_categories                      78035 non-null object\n",
      "project_subject_subcategories                   78035 non-null object\n",
      "project_title                                   78035 non-null object\n",
      "project_essay_1                                 78035 non-null object\n",
      "project_essay_2                                 78035 non-null object\n",
      "project_essay_3                                 2704 non-null object\n",
      "project_essay_4                                 2704 non-null object\n",
      "project_resource_summary                        78035 non-null object\n",
      "teacher_number_of_previously_posted_projects    78035 non-null int64\n",
      "dtypes: int64(1), object(14)\n",
      "memory usage: 8.9+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3018: DtypeWarning: Columns (11,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#train_df = pd.read_csv(\"./Data/train.csv\")\n",
    "train_df = pd.read_csv(\"./Data/test.csv\")\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def noun_stem_analyzer(string):\n",
    "    st = nltk.stem.lancaster.LancasterStemmer()\n",
    "    return [st.stem(word) for word, pos in nltk.pos_tag(\n",
    "            nltk.word_tokenize(string)) if pos in [\"NNS\", \"VBP\", \"VB\", \"NNP\", \"JJ\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from my_modules.my_encoder import CategoryValueEncoder as CVE\n",
    "from my_modules.my_encoder import TextValueEncoder as TVE\n",
    "from my_modules.my_encoder import DateValueEncoder as DVE\n",
    "from my_modules.my_encoder import NumericValueEncoder as NVE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df1 = train_df['project_essay_1']\n",
    "enc1 = TVE(df1[0:10000])\n",
    "#enc1 = TVE(df1)\n",
    "# 出現頻度が1%未満のデータは無視する。\n",
    "# 理由：1回でた単語のデータのラベルが正であっても負であっても、その単語との因果関係を認められないから。\n",
    "enc1.vectorizer = CountVectorizer(min_df=0.01, max_df=1.00, stop_words=\"english\", analyzer=noun_stem_analyzer) # stop_words=\"english\",\n",
    "df2 = enc1.to_bow_encoding()\n",
    "names = enc1.vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'m\",\n",
       " \"'re\",\n",
       " \"'ve\",\n",
       " '21st',\n",
       " '\\\\',\n",
       " '\\\\r\\\\n',\n",
       " '\\\\r\\\\n\\\\r\\\\n',\n",
       " '\\\\r\\\\n\\\\r\\\\ni',\n",
       " '\\\\r\\\\n\\\\r\\\\nmy',\n",
       " '\\\\r\\\\n\\\\r\\\\nour',\n",
       " '\\\\r\\\\n\\\\r\\\\nthe',\n",
       " '\\\\r\\\\ni',\n",
       " '\\\\r\\\\nmy',\n",
       " '\\\\r\\\\nour',\n",
       " 'abl',\n",
       " 'academ',\n",
       " 'accompl',\n",
       " 'achiev',\n",
       " 'act',\n",
       " 'ad',\n",
       " 'addit',\n",
       " 'adult',\n",
       " 'adv',\n",
       " 'afford',\n",
       " 'afr',\n",
       " 'allow',\n",
       " 'am',\n",
       " 'amaz',\n",
       " 'apply',\n",
       " 'apprecy',\n",
       " 'ar',\n",
       " 'area',\n",
       " 'art',\n",
       " 'ask',\n",
       " 'attend',\n",
       " 'aut',\n",
       " 'avail',\n",
       " 'awesom',\n",
       " 'background',\n",
       " 'bas',\n",
       " 'be',\n",
       " 'beauty',\n",
       " 'becom',\n",
       " 'begin',\n",
       " 'behavy',\n",
       " 'believ',\n",
       " 'benefit',\n",
       " 'big',\n",
       " 'bil',\n",
       " 'book',\n",
       " 'boy',\n",
       " 'bright',\n",
       " 'bring',\n",
       " 'build',\n",
       " 'busy',\n",
       " 'cap',\n",
       " 'car',\n",
       " 'cent',\n",
       " 'chair',\n",
       " 'challeng',\n",
       " 'chang',\n",
       " 'childr',\n",
       " 'cho',\n",
       " 'choos',\n",
       " 'circumst',\n",
       " 'city',\n",
       " 'class',\n",
       " 'classroom',\n",
       " 'clos',\n",
       " 'collab',\n",
       " 'com',\n",
       " 'comfort',\n",
       " 'common',\n",
       " 'commun',\n",
       " 'complet',\n",
       " 'comput',\n",
       " 'conceiv',\n",
       " 'connect',\n",
       " 'continu',\n",
       " 'control',\n",
       " 'country',\n",
       " 'cre',\n",
       " 'crit',\n",
       " 'cult',\n",
       " 'cur',\n",
       " 'cury',\n",
       " 'dai',\n",
       " 'day',\n",
       " 'deserv',\n",
       " 'desk',\n",
       " 'develop',\n",
       " 'diff',\n",
       " 'difficult',\n",
       " 'dis',\n",
       " 'discov',\n",
       " 'discuss',\n",
       " 'divers',\n",
       " 'do',\n",
       " 'don',\n",
       " 'dream',\n",
       " 'due',\n",
       " 'eag',\n",
       " 'ear',\n",
       " 'econom',\n",
       " 'educ',\n",
       " 'el',\n",
       " 'emot',\n",
       " 'enco',\n",
       " 'energet',\n",
       " 'eng',\n",
       " 'engl',\n",
       " 'enh',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'ens',\n",
       " 'ent',\n",
       " 'enthusiast',\n",
       " 'entir',\n",
       " 'environ',\n",
       " 'ess',\n",
       " 'ethn',\n",
       " 'ev',\n",
       " 'everyday',\n",
       " 'excel',\n",
       " 'excit',\n",
       " 'expect',\n",
       " 'expery',\n",
       " 'expl',\n",
       " 'express',\n",
       " 'extr',\n",
       " 'fac',\n",
       " 'famy',\n",
       " 'favorit',\n",
       " 'feel',\n",
       " 'few',\n",
       " 'fif',\n",
       " 'fin',\n",
       " 'find',\n",
       " 'first',\n",
       " 'flex',\n",
       " 'foc',\n",
       " 'form',\n",
       " 'fost',\n",
       " 'four',\n",
       " 'fre',\n",
       " 'friend',\n",
       " 'ful',\n",
       " 'fun',\n",
       " 'fund',\n",
       " 'fut',\n",
       " 'gain',\n",
       " 'gam',\n",
       " 'gen',\n",
       " 'get',\n",
       " 'girl',\n",
       " 'giv',\n",
       " 'go',\n",
       " 'goal',\n",
       " 'good',\n",
       " 'grad',\n",
       " 'gre',\n",
       " 'group',\n",
       " 'grow',\n",
       " 'hand',\n",
       " 'hands-on',\n",
       " 'hap',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hav',\n",
       " 'hear',\n",
       " 'help',\n",
       " 'high',\n",
       " 'hisp',\n",
       " 'hom',\n",
       " 'hop',\n",
       " 'hour',\n",
       " 'household',\n",
       " 'hug',\n",
       " 'hungry',\n",
       " 'idea',\n",
       " 'imagin',\n",
       " 'import',\n",
       " 'improv',\n",
       " 'in',\n",
       " 'includ',\n",
       " 'incorp',\n",
       " 'increas',\n",
       " 'incred',\n",
       " 'independ',\n",
       " 'individ',\n",
       " 'innov',\n",
       " 'inquisit',\n",
       " 'inspir',\n",
       " 'interact',\n",
       " 'interest',\n",
       " 'ipad',\n",
       " 'issu',\n",
       " 'item',\n",
       " 'job',\n",
       " 'keep',\n",
       " 'kid',\n",
       " 'kindergart',\n",
       " 'know',\n",
       " 'lack',\n",
       " 'langu',\n",
       " 'larg',\n",
       " 'last',\n",
       " 'lead',\n",
       " 'learn',\n",
       " 'leav',\n",
       " 'lesson',\n",
       " 'let',\n",
       " 'level',\n",
       " 'lifelong',\n",
       " 'lik',\n",
       " 'limit',\n",
       " 'littl',\n",
       " 'liv',\n",
       " 'loc',\n",
       " 'long',\n",
       " 'look',\n",
       " 'lot',\n",
       " 'lov',\n",
       " 'low',\n",
       " 'low-income',\n",
       " 'low-income/high',\n",
       " 'lucky',\n",
       " 'lunch',\n",
       " 'mak',\n",
       " 'many',\n",
       " 'mat',\n",
       " 'math',\n",
       " 'mathem',\n",
       " 'meal',\n",
       " 'mean',\n",
       " 'meet',\n",
       " 'memb',\n",
       " 'middl',\n",
       " 'milit',\n",
       " 'mind',\n",
       " 'mot',\n",
       " 'mov',\n",
       " 'much',\n",
       " 'multipl',\n",
       " 'my',\n",
       " 'nat',\n",
       " 'necess',\n",
       " 'nee',\n",
       " 'neighb',\n",
       " 'new',\n",
       " 'next',\n",
       " 'obstac',\n",
       " 'off',\n",
       " 'old',\n",
       " 'on',\n",
       " 'op',\n",
       " 'opportun',\n",
       " 'opt',\n",
       " 'oth',\n",
       " 'overcom',\n",
       " 'own',\n",
       " 'par',\n",
       " 'particip',\n",
       " 'pass',\n",
       " 'past',\n",
       " 'peer',\n",
       " 'peopl',\n",
       " 'person',\n",
       " 'phys',\n",
       " 'plac',\n",
       " 'play',\n",
       " 'pleas',\n",
       " 'posit',\n",
       " 'poss',\n",
       " 'pot',\n",
       " 'pre-k',\n",
       " 'prep',\n",
       " 'pres',\n",
       " 'problem',\n",
       " 'produc',\n",
       " 'program',\n",
       " 'project',\n",
       " 'proud',\n",
       " 'provid',\n",
       " 'publ',\n",
       " 'put',\n",
       " 'qual',\n",
       " 'quest',\n",
       " 'rang',\n",
       " 'reach',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'real',\n",
       " 'receiv',\n",
       " 'reduc',\n",
       " 'regul',\n",
       " 'rel',\n",
       " 'rememb',\n",
       " 'requir',\n",
       " 'resourc',\n",
       " 'respect',\n",
       " 'respons',\n",
       " 'rich',\n",
       " 'rig',\n",
       " 'right',\n",
       " 'rur',\n",
       " 'saf',\n",
       " 'sam',\n",
       " 'say',\n",
       " 'scholars',\n",
       " 'school',\n",
       " 'schools',\n",
       " 'sci',\n",
       " 'second',\n",
       " 'see',\n",
       " 'sens',\n",
       " 'serv',\n",
       " 'set',\n",
       " 'sev',\n",
       " 'show',\n",
       " 'simpl',\n",
       " 'singl',\n",
       " 'sit',\n",
       " 'situ',\n",
       " 'skil',\n",
       " 'smal',\n",
       " 'smart',\n",
       " 'smil',\n",
       " 'soc',\n",
       " 'socio-economic',\n",
       " 'socioeconom',\n",
       " 'solv',\n",
       " 'sou',\n",
       " 'span',\n",
       " 'speak',\n",
       " 'spec',\n",
       " 'spend',\n",
       " 'sport',\n",
       " 'standard',\n",
       " 'start',\n",
       " 'stat',\n",
       " 'stay',\n",
       " 'stem',\n",
       " 'story',\n",
       " 'strive',\n",
       " 'strong',\n",
       " 'struggle',\n",
       " 'struggles',\n",
       " 'stud',\n",
       " 'study',\n",
       " 'styl',\n",
       " 'subject',\n",
       " 'success',\n",
       " 'such',\n",
       " 'supply',\n",
       " 'support',\n",
       " 'sur',\n",
       " 'tak',\n",
       " 'tal',\n",
       " 'teach',\n",
       " 'technolog',\n",
       " 'tel',\n",
       " 'thank',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'third',\n",
       " 'thought',\n",
       " 'thrive',\n",
       " 'tim',\n",
       " 'titl',\n",
       " 'tool',\n",
       " 'top',\n",
       " 'tradit',\n",
       " 'tru',\n",
       " 'try',\n",
       " 'typ',\n",
       " 'un',\n",
       " 'understand',\n",
       " 'unit',\n",
       " 'urb',\n",
       " 'us',\n",
       " 'vary',\n",
       " 'vis',\n",
       " 'wait',\n",
       " 'walk',\n",
       " 'want',\n",
       " 'way',\n",
       " 'welcom',\n",
       " 'whol',\n",
       " 'wid',\n",
       " 'wil',\n",
       " 'wond',\n",
       " 'word',\n",
       " 'work',\n",
       " 'writ',\n",
       " 'year',\n",
       " 'young',\n",
       " '’',\n",
       " '”']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from my_modules.my_encoder import CategoryValueEncoder as CVE\n",
    "from my_modules.my_encoder import TextValueEncoder as TVE\n",
    "from my_modules.my_encoder import DateValueEncoder as DVE\n",
    "from my_modules.my_encoder import NumericValueEncoder as NVE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df1 = train_df['project_title']\n",
    "#enc1 = TVE(df1[0:30000])\n",
    "enc1 = TVE(df1)\n",
    "# 出現頻度が1%未満のデータは無視する。\n",
    "# 理由：1回でた単語のデータのラベルが正であっても負であっても、その単語との因果関係を認められないから。\n",
    "enc1.vectorizer = CountVectorizer(min_df=0.01, max_df=1.00, stop_words=\"english\") # We get ValueError when get any data. You should enhance range. \n",
    "df2 = enc1.to_bow_encoding()\n",
    "names = enc1.vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame({'id': ['A01', 'A01', 'A02', 'A03'],\n",
    "                     'description': ['apple', 'amazon', 'google', 'facebook'],\n",
    "                     'quantity': ['3', '2', '4', '10'],\n",
    "                     'price': ['80', '100', '90', '70']})\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#カテゴリ変数の変換\n",
    "hot_list = ['A02', 'A03']\n",
    "train_df['quantity'] = train_df['id'].apply(lambda x: 1 if x in hot_list else 0)\n",
    "#train_df['quantity'] = train_df['id'].map( {'A01': 0, 'A02': 1, 'A03': 2} ).astype(int)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Most', 'of', 'my', 'kindergarten', 'students', 'come', 'from', 'low']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Most of my kindergarten students come from low\"\n",
    "words = nltk.word_tokenize(string)\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Most', 'JJS'),\n",
       " ('of', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('kindergarten', 'JJ'),\n",
       " ('students', 'NNS'),\n",
       " ('come', 'VBP'),\n",
       " ('from', 'IN'),\n",
       " ('low', 'JJ')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.pos_tag(words)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anser = noun_stem_analyzer(string)\n",
    "anser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB # ガウシアン\n",
    "\n",
    "X = np.array([[1,2,3,4,5,6,7,8],\n",
    "              [1,1,3,4,5,6,6,7],\n",
    "              [2,1,2,4,5,8,8,8]]) # 特徴ベクトル\n",
    "y = np.array([1, 2, 3]) # そのラベル\n",
    "t = np.array([[2,2,4,5,6,8,8,8],\n",
    "              [2,2,4,5,6,0,0,0]]) # テストデータ\n",
    "\n",
    "clf = GaussianNB() # 正規分布を仮定したベイズ分類\n",
    "clf.fit(X, y) # 学習をする\n",
    "clf.predict(t) # => [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X, y)\n",
    "clf.predict(t) #=> [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "clf.fit(X, y)\n",
    "clf.predict(t) #=> [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
